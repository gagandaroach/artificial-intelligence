{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "source": [
    "### Reinforcement Learning Tutorial with OpenAI Gym\n",
    "\n",
    "Jay Urbain, PhD  \n",
    "\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms that includes many game environments.\n",
    "\n",
    "This notebook provides a tutorial with example implementations for using the OpenAI Gym environment:\n",
    "- Interacting with Gym.  \n",
    "- Value iteration in deterministic environments.  \n",
    "- Q-learning in deterministic environments.  \n",
    "- Q-learning in non-determinisitc environments.  \n",
    "- **ON YOUR OWN:** Complete Q-Learning for Gym environment of your choice.   \n",
    "\n",
    "References:  \n",
    "https://gym.openai.com/  \n",
    "https://www.kaggle.com/kernels/scriptcontent/6183449/notebook  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, review the Gym toolkit and sample environments:   \n",
    "    \n",
    "https://gym.openai.com/   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import sleep\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a371168b9fe9d579fd65f95a808998ba009127d7"
   },
   "source": [
    "#### Gym\n",
    "\n",
    "There are  many games that are available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43f7aa21cb73928f74f64a6bb768c67784beddab"
   },
   "outputs": [],
   "source": [
    "print(envs.registry.all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ee2e2c8c9e58a4de97d163f262691ad3273a311"
   },
   "source": [
    "We can start with a basic game called Taxi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80a92d7ae756846e1c72846c47209899395175c6"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24fa3bba1a709611174be39b512dcdddcbae66a4"
   },
   "source": [
    "#### Taxi-v2\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "[Dietterich2000] T Erez, Y Tassa, E Todorov, \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\", 2011.\n",
    "\n",
    "Actions: \n",
    "There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "Rewards: \n",
    "There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "    \n",
    "Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters: locations\n",
    "\n",
    "https://gym.openai.com/envs/Taxi-v2/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7650831ece2de8bea1595b76d446c6dec9a80ba1"
   },
   "source": [
    "#### Interacting with the Gym environment  \n",
    "\n",
    "The OpenAI Gym toolkit follows a standard RL/Markov Decision Process (MDP) for handling interactions with the game.   \n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*7Ae4mf9gVvpuMgenwtf8wA.png\">   \n",
    "Source: [OpenAI](https://openai.com/)   \n",
    "\n",
    "At each timestep, the agent chooses an action, and the environment returns an observation and a reward:  \n",
    "\n",
    "*observation, reward, done, info = env.step(action)*    \n",
    "* observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game like Taxi.\n",
    "* reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* done (boolean): whether itâ€™s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "* info (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.  \n",
    "\n",
    "To illustrate interacting with the enviornment, we can do some random steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77dd8855ce714bfaf0f50fe0a2c1c6d4069b37b7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's first do some random steps in the game so you see how the game looks like\n",
    "\n",
    "rew_tot=0\n",
    "obs= env.reset() # always reset the environmebnt\n",
    "env.render() # display initial environment state\n",
    "for _ in range(6):\n",
    "    action = env.action_space.sample() # sample random action from possible actions (action_space)\n",
    "    obs, rew, done, info = env.step(action) # execute action in the environment\n",
    "    rew_tot = rew_tot + rew # add to cumulative reward\n",
    "    env.render() # render update environment state\n",
    "\n",
    "#Print the reward of these random action\n",
    "print(\"Reward: %r\" % rew_tot)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba589475e35bf258a87335d43c0cbee595f8bc7d"
   },
   "source": [
    "#### Actions  \n",
    "Action (a): the action the agent provides to the environment. \n",
    "\n",
    "env.action_space defines the set of environment actions available to the agen. tell you\n",
    "\n",
    "Actions available to the Taxi game $[0..5]$:      \n",
    "* 0: move south\n",
    "* 1: move north\n",
    "* 2: move east \n",
    "* 3: move west \n",
    "* 4: pickup passenger\n",
    "* 5: dropoff passenger\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "500b6e1fabcceccf19ad494149d287fa82bf65c5"
   },
   "outputs": [],
   "source": [
    "# action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out\n",
    "print(env.action_space)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "print(\"Possible actions: [0..%a]\" % (NUM_ACTIONS-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bcee395c56b5c161e43ce36832c0c3673e8cd3f"
   },
   "source": [
    "#### State   \n",
    "State (s): Represents the board state of the game and is returned as the observation. \n",
    "\n",
    "In the Taxi game, the observation is an integer, one of 500 possible states. Each state can be translated into a graphic with the render function. \n",
    "\n",
    "*Note: this is specific for the Taxi game. In an Atari style game the observation is the game screen with many coloured pixels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeeb040b8bee18af1c5c070b573912eabe4e139f"
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "env.env.s=42 # some random number, you might recognize it\n",
    "env.render()\n",
    "env.env.s = 222 # and some other\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13c4565e8ab253c842d69d1f3d02973b3c0d95f5"
   },
   "source": [
    "#### Markov decision process(MDP)\n",
    "The Taxi game is an example of an [Markov decision process ](https://en.wikipedia.org/wiki/Markov_decision_process). The game can be described in states, possible actions in a state (leading to a next state with a certain probability) and rewards associated with that state transition.\n",
    "\n",
    "A [Markovian property](https://en.wikipedia.org/wiki/Markov_property) means that the current state encapsulates all prior information.\n",
    "\n",
    "The Reinforcement Learning environment is modeled as an MDP. Given this environment, the agent takes actions to maximize the cumulative reward. Since the internal workings of the environment is essentially a \"black box,\" it can be referred to as a `hidden markov model` that we will learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2a61906ac33e831e11d0c70b011478c8c2f3124"
   },
   "source": [
    "#### Policy   \n",
    "\n",
    "Policy ($\\pi$): The strategy that the agent uses to determine the next action `a` to take in state `s`. \n",
    "\n",
    "The optimal policy ($\\pi^*$), is the policy that maximizes the expected cumulative reward. \n",
    "\n",
    "Our goal is to learn $\\pi^*$ by solvoing the Bellman equation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65441f069d3f6afa07cde0ce37527d956288f601"
   },
   "source": [
    "#### Bellman equation  \n",
    "\n",
    "$V^*(s) \\leftarrow max_a\\sum_{s'}P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "where\n",
    "* *R(s,a,s')* - Reward for action a in state s, transitioning to s'.\n",
    "* *P(s'|s,a)* - Probability (expectation) of going to state s' given action a in state s. The Taxi game actions are deterministic so the probability that selected action will lead to expected state is 100%. \n",
    "* $\\gamma$ - Discount rate for future rewards. It must be between 0 and <1. The higher gamma the more focus on long term rewards. May not converge if $\\gamma=1$.\n",
    "\n",
    "The value iteration algorithm:  \n",
    "* $V(s)$ represents the cumulative reward for state $s$. $V_{\\pi}(s)$ is the expected cumulative reward of the current state $s$ sunder policy $\\pi$.  \n",
    "\n",
    "The Q learning algorithm:   \n",
    "* The action-value $Q(s,a)$ function represents the cumulative reward of the current state $s$ and taking action $a$ under policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00d5812c19f9bb264e4c8859d4ea7084bbcbd2c4"
   },
   "source": [
    "#### Value iteration algorithm   \n",
    "\n",
    "The idea is to iteratively calculate the value (expected long-term cumulative reward) for each state. The algorithm iterates over all states $s$ and possible actions $a$ to explore the value (cumulative discounted rewards) $V[s]$ for a given state $s$. \n",
    "\n",
    "The algorithm iteratess until $V[s]$ converges. The Optimal policy $\\pi^*$ is the action taken at each state $s$ that maximizes the value. This value iteration algorithm is an example of [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60c2f7e729ac11e8696694eafa4ac9ed944d4665"
   },
   "outputs": [],
   "source": [
    "# Value iteration algorithm\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "V = np.zeros([NUM_STATES]) # Value for each state\n",
    "Pi = np.zeros([NUM_STATES], dtype=int)  # policy, iteratively updated, to get the optimal policy\n",
    "gamma = 0.9 # discount factor\n",
    "significant_improvement = 0.01\n",
    "\n",
    "def best_action_value(s):\n",
    "    # finds the highest value action (max_a) in state s\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    # iterate through all possible actions to find the best current action\n",
    "    for a in range (0, NUM_ACTIONS):\n",
    "        env.env.s = s\n",
    "        s_new, rew, done, info = env.step(a) #take the action\n",
    "        v = rew + gamma * V[s_new]\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    return best_a\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    # biggest_change - delta\n",
    "    delta = 0\n",
    "    for s in range (0, NUM_STATES):\n",
    "        old_v = V[s]\n",
    "        action = best_action_value(s) # choose an action with the highest future reward\n",
    "        env.env.s = s # goto the state\n",
    "        s_new, reward, done, info = env.step(action) #take the action\n",
    "        V[s] = reward + gamma * V[s_new] # update Value for the state using Bellman equation\n",
    "        Pi[s] = action\n",
    "        delta = max(delta, np.abs(old_v - V[s]))\n",
    "    iteration += 1\n",
    "    if delta < significant_improvement:\n",
    "        print (iteration,' iterations done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "373b433ca66d1dee88071f55b414f5344ab80ecd"
   },
   "outputs": [],
   "source": [
    "# Review how the algorithm solves the taxi game\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = Pi[obs]\n",
    "    obs, rew, done, info = env.step(action) # take step using selected action\n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "# Print the reward of these actions\n",
    "print(\"Reward: %r\" % rew_tot)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db36d8f42216986bd8778dae5c86bdb022193322"
   },
   "source": [
    "#### Model vs Model-free based methods  \n",
    "\n",
    "Value iteration solves the Taxi game. However, we have to know all environment states/transitions upfront so the algorithm works. In Reinforcement Learning, this is refered to as a model based method.   \n",
    "\n",
    "If all states are not known upfront, we can learn states and actions during learning. This is refered to as a model-free method.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f99c26b731ef1829788435ba4a3600b9f531546"
   },
   "source": [
    "#### Basic Q-learning algorithm    \n",
    "In the [Q-learning](https://en.wikipedia.org/wiki/Q-learning) algorithm, the agent (Taxi) interacts with its environment to update its knowledge about the model so it can learn an optimal policy.\n",
    "\n",
    "The $Q-matrix Q(s,a)$ is used to store the current maximum discounted future reward when the agent performs an action $a$ in state $s$. $Q(s, a)$ provides estimates for the best course of action for a given $a$ in state $s$. Upon convergence, the optimal policy $\\po^*$ can be read from the $Q-matrix$. t\n",
    " \n",
    "After every step we update $Q(s,a)$ using the reward and the max $Q-value$ for new state resulting from the action. This update is done using the action-value form of the Bellman equation.   \n",
    "\n",
    "$Q_{t+1}(s_t,a_t) = Q_{t}(s_t,a_t) + \\alpha_t(s_t,a_t) * [R_{t+1} + \\gamma * max_a Q_t(s_{t+1},a_t) - Q_t(s_t,a_t)]$\n",
    "\n",
    "Notes: \n",
    "- Q-learning was the basis for Deep Q-learning (Deep referring to Neural Network technology)  \n",
    "- [Temporal difference learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) and [Sarsa](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action) algorithems explored simular value expressions. . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a8bf6c408b00cdc614614159f001d63617f1db5"
   },
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\n",
    "gamma = 0.9 # discount factor\n",
    "alpha = 0.9 # learning rate\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    reward_total = 0\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "            action = np.argmax(Q[obs]) #choosing the action with the highest Q value \n",
    "            obs2, reward, done, info = env.step(action) #take the action\n",
    "            Q[obs,action] += alpha * (reward + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n",
    "            #Q[obs,action] = rew + gamma * np.max(Q[obs2]) # same equation but with learning rate = 1 returns the basic Bellman equation\n",
    "            reward_total = reward_total + reward\n",
    "            obs = obs2   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode, reward_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90fa1d8270ca776ecc23badc8ef43e22a71e7e52"
   },
   "source": [
    "So, what is the magic, how does it solve it? \n",
    "\n",
    "The Q-matrix is initialized with zero's. So initially it starts moving randomly until it hits a state/action with rewards or state/actions with a penalty. For understanding, let's simplify the problem that it needs to go to a certain drop-off position to get a reward. So random moves get no rewards but by luck (brute force enough tries) the state/action is found where a reward is given. So next game the immediate actions preceding this state/action will direct toward it by use of the Q-Matrix. The next iteration the actions before that, etc, etc. In other words, it solves \"the puzzle\" backwards from end-result (drop-off passenger) towards steps to be taken to get there in a iterative fashion.  \n",
    "\n",
    "Note that in case of the Taxi game there is a reward of -1 for each action. So if in a state the algorithm explored eg south which let to no value the Q-matrix is updated to -1 so next iteration (because values were initialized on 0) it will try an action that is not yet tried and still on 0. So also by design it encourages systematic exploration of states and actions \n",
    "\n",
    "If you put the learning rate on 1 the game also solves. Reason is that there is only one reward (dropoff passenger), so the algorithm will find it whatever learning rate. In case a game has more reward places the learning rate determines if it should prioritize longer term or shorter term rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e4ac4c4a5aea696de188db3159b880193994709"
   },
   "outputs": [],
   "source": [
    "# Let's see how the algorithm solves the taxi game by following the policy to take actions delivering max value\n",
    "\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "print(\"Reward: %r\" % rew_tot)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3ff1014ef78d57017e4966acdf3fc235fa7eb86"
   },
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "The taxi game operates in a deterministic environment one terminal state with the reward: dropoff passenger, receive +20. \n",
    "\n",
    "100% of the time, our algorithm *exploits* action = np.argmax(Q[obs]). To deal with more complex environments, we need to update our algorithm to explore. This is called the tradeoff between \"exploitation\" and \"exploration\".\n",
    "* Exploitation: Make the best decision given current information (Go to the restaurant you know you like)\n",
    "* Exploration: Gather more information (Try a new restaurant)\n",
    "\n",
    "Approaches:  \n",
    "Epsilon Greedy  \n",
    "* Exploit with probability $(1â€Šâ€”â€Š\\epsilon)$ and explore probability $\\epsilon$, the rates of exploration and exploitation are fixed.\n",
    "Epsilon-Decreasing  \n",
    "* Epsilon Greedy with epsilon decreasing over time. \n",
    "Thompson sampling  \n",
    "* The rates of exploration and exploitation are dynamically updated with respect to the entire probability distribution.   \n",
    "Epsilon-Decreasing with Softmax  \n",
    "* Epsilon-Decreasing, however in the case of exploring a new option, we donâ€™t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that (this is the softmax part).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7d5a310b7ba811af48c10e8bee933501dd10cf9"
   },
   "source": [
    "#### [Frozen lakes](https://gym.openai.com/envs/FrozenLake-v0/) of OpenAI/Gym.  \n",
    "\n",
    "Frozen lakes provides simple non-deterministic envrionment.\n",
    "\n",
    "Description: \"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"  \n",
    "\n",
    "Notice that the game is not deterministic anymore: \"won't always move in the direction you intend\". Note it is really slippery, the chance you move in the direction you want is relatively small.\n",
    "\n",
    "S- Start  \n",
    "G - Goal  \n",
    "F- Frozen (safe)  \n",
    "H- Hole (dead)  \n",
    "\n",
    "Game layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ea12b3040c3d1c9629a9a47931ad6aa616f93a9"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b14f7d3622c01a3e55a6c1bab79cc65b2fc61dd"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\n",
    "gamma = 0.95 # discount factor\n",
    "alpha = 0.01 # learning rate\n",
    "epsilon = 0.1 #\n",
    "for episode in range(1,500001):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            # exploration with a new option with probability epsilon, the epsilon greedy approach\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # exploitation\n",
    "            action = np.argmax(Q[obs])\n",
    "        obs2, reward, done, info = env.step(action) #take the action\n",
    "        Q[obs,action] += alpha * (reward + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n",
    "        obs = obs2   \n",
    "        \n",
    "    if episode % 5000 == 0:\n",
    "        #report every 5000 steps, test 100 games to get avarage point score for statistics and verify if it is solved\n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs= env.reset()\n",
    "            done=False\n",
    "            while done != True: \n",
    "                action = np.argmax(Q[obs])\n",
    "                obs, rew, done, info = env.step(action) #take step using selected action\n",
    "                rew_average += rew\n",
    "        rew_average=rew_average/100\n",
    "        print('Episode {} avarage reward: {}'.format(episode,rew_average))\n",
    "        \n",
    "        if rew_average > 0.8:\n",
    "            # FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n",
    "            # Test it on 0.8 so it is not a one-off lucky shot solving it\n",
    "            print(\"Frozen lake solved\")\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "570a20a4336cb40b424d9a5f68420b61c5dcb0a4"
   },
   "outputs": [],
   "source": [
    "# Let's see how the algorithm solves the frozen-lakes game\n",
    "\n",
    "rew_tot=0.\n",
    "obs= env.reset()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot += rew\n",
    "    env.render()\n",
    "\n",
    "print(\"Reward:\", rew_tot)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ecaf0ce4d16b049d6390226c4cf39a8e3a7db9c"
   },
   "source": [
    "It appears that if you move right there is a significant chance you move up or down, and if you attempt to move up there is a significant chance you move left or right, etc. So the algorithm learned that if you are on the frozen tile left column second row and you want to move down it is risky to give the down command because you could move to the right into the hole. So it gives the left command because if will keep you on the tile or move you up or down, but not to thr right.  \n",
    "Or in other words, the algorithm learned to take that actions with the least risk to (accidently slip) drown into a hole. Also interesting to se it learned as first move to go left, this to avoid you move right which is the more dangerous road.  \n",
    "\n",
    "Note: there is no 100% score possible. By consitently moving away from a hole you can safely traverse all fields except 1 (second row, third column) on which you could glide into due to slippery ice.  \n",
    "\n",
    "Also good to notice the algorithm uses tenthousands of iterations to find the optimal policy, while this is a 4 by 4 playing field..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ON YOUR OWN:**\n",
    "    \n",
    "Explore the OpenAI Gym environments: https://gym.openai.com/envs\n",
    "        \n",
    "Implement Q-Learning for at least one of the environments of your choice. It can be a simple environment. Avoid the highly graphical Atari environments, we will cover that with the next tutorial on Deep Q-Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
